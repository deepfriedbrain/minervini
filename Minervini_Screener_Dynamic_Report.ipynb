{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8aae94",
   "metadata": {},
   "source": [
    "Source Repo: https://github.com/wholidi/Project/tree/main/Minervini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b55bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "BASE_DATA_DIR = \"yfinance_data\"\n",
    "END_DATE = \"2025-09-18\" # The end date is inclusive\n",
    "TIME_PERIOD_DAYS = 365\n",
    "INDEX_SYMBOL = \"SPY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3368bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "TIME_FORMAT = \"%Y-%m-%d\"\n",
    "FILE_TIME_FORMAT = TIME_FORMAT.replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_file_date_str(date_str: str):\n",
    "    date_dt = pd.to_datetime(date_str)\n",
    "    return date_dt.strftime(FILE_TIME_FORMAT)\n",
    "\n",
    "def normalize_ticker(ticker: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Wikipedia tickers to Yahoo Finance style:\n",
    "    - Remove footnotes like [1], [a]\n",
    "    - Convert BRK.B -> BRK-B, BF.B -> BF-B\n",
    "    \"\"\"\n",
    "    ticker = re.sub(r\"\\[.*?\\]\", \"\", ticker)  # remove footnotes\n",
    "    ticker = ticker.replace(\".\", \"-\")\n",
    "    return ticker.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db898083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "def get_sp500_tickers_on(date_str: str):\n",
    "    \"\"\"\n",
    "    Return the list of S&P 500 tickers as of the given date (YYYY-MM-DD),\n",
    "    and save full table with details to CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch Wikipedia page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    resp = requests.get(URL, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Read tables\n",
    "    html_io = StringIO(resp.text)\n",
    "    tables = pd.read_html(html_io, flavor=\"lxml\")\n",
    "\n",
    "    # --- Current constituents table ---\n",
    "    df = tables[0]\n",
    "    df['Symbol'] = df['Symbol'].astype(str).map(normalize_ticker)\n",
    "\n",
    "    # Ensure all expected columns exist\n",
    "    expected_cols = [\n",
    "        \"Symbol\",\"Security\",\"GICS Sector\",\"GICS Sub-Industry\",\n",
    "        \"Headquarters Location\",\"Date added\",\"CIK\",\"Founded\"\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # --- Changes table ---\n",
    "    changes_df = None\n",
    "    for table in tables:\n",
    "        if isinstance(table.columns, pd.MultiIndex):\n",
    "            table.columns = [\"_\".join([str(i) for i in col if i]).strip() for col in table.columns]\n",
    "        cols_lower = [c.lower() for c in table.columns]\n",
    "        if \"ticker\" in \"\".join(cols_lower) and \"added\" in \"\".join(cols_lower):\n",
    "            changes_df = table.copy()\n",
    "            break\n",
    "\n",
    "    if changes_df is None:\n",
    "        raise ValueError(\"Could not find the 'Selected changes to the list of S&P 500 components' table\")\n",
    "\n",
    "    date_col = [c for c in changes_df.columns if \"date\" in c.lower()][0]\n",
    "    added_col = [c for c in changes_df.columns if \"added\" in c.lower() and \"ticker\" in c.lower()][0]\n",
    "    removed_col = [c for c in changes_df.columns if \"removed\" in c.lower() and \"ticker\" in c.lower()][0]\n",
    "\n",
    "    changes_df[date_col] = pd.to_datetime(changes_df[date_col], errors=\"coerce\")\n",
    "    changes_df = changes_df.dropna(subset=[date_col])\n",
    "    changes_df = changes_df.sort_values(date_col, ascending=False)\n",
    "\n",
    "    # Current tickers set\n",
    "    tickers = set(df['Symbol'])\n",
    "\n",
    "    target_date = pd.to_datetime(date_str)\n",
    "\n",
    "    # Roll back changes after target_date\n",
    "    for _, row in changes_df.iterrows():\n",
    "        change_date = row[date_col]\n",
    "        if change_date <= target_date:\n",
    "            break\n",
    "\n",
    "        added = normalize_ticker(str(row.get(added_col, \"\")))\n",
    "        removed = normalize_ticker(str(row.get(removed_col, \"\")))\n",
    "\n",
    "        if added and added in tickers:\n",
    "            tickers.remove(added)\n",
    "        if removed:\n",
    "            tickers.add(removed)\n",
    "            # Fill missing details for removed ticker if not in current df\n",
    "            if removed not in df['Symbol'].values:\n",
    "                new_row = {col: \"\" for col in expected_cols}\n",
    "                new_row[\"Symbol\"] = removed\n",
    "                new_row[\"Security\"] = str(row.get(\"Removed_Security\", \"\"))\n",
    "                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    # Filter df to tickers that exist on target date\n",
    "    final_df = df[df['Symbol'].isin(tickers)].copy()\n",
    "    final_df = final_df[expected_cols]\n",
    "\n",
    "    # üîπ Save full table with sanitized tickers to CSV\n",
    "    TICKERS_FILE = f\"sp500_{get_file_date_str(date_str)}.csv\"\n",
    "    final_df.to_csv(TICKERS_FILE, index=False)\n",
    "    print(f\"‚úÖ Saved full S&P 500 table with sanitized tickers to {TICKERS_FILE}\")\n",
    "\n",
    "    # üîπ Return list of tickers\n",
    "    return sorted(final_df['Symbol'].dropna().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers_on(END_DATE)\n",
    "print(f\"S&P 500 tickers on {END_DATE}: {len(tickers)} symbols\")\n",
    "print(tickers[:20], \"...\")\n",
    "# Check if PLTR is present in tickers\n",
    "if \"PLTR\" in tickers:\n",
    "    print(\"PLTR is present in tickers\")\n",
    "else:\n",
    "    print(\"PLTR is not present in tickers on_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_yfinance_data(\n",
    "    symbol: str, \n",
    "    start_date: str, \n",
    "    end_date: str, \n",
    "    interval: str = \"1d\", \n",
    "    save_to_file: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch historical OHLCV data for a given ticker from Yahoo Finance, \n",
    "    optionally using a cached file.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): Ticker symbol (e.g., \"AAPL\", \"SPY\").\n",
    "        start_date (str): Start date for historical data in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date for historical data in 'YYYY-MM-DD' format.\n",
    "        interval (str): Data interval (\"1d\", \"1wk\", \"1mo\", etc.). Default is \"1d\".\n",
    "        save_to_file (bool): Whether to save/load data from cache. Default = True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with timestamp index and columns: open, high, low, close, volume.\n",
    "                      Returns empty DataFrame if no data is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # üîπ Always make `end_date` inclusive by adding +1 day\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    end_date_exclusive = end_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # üîπ Normalize dates to use underscores instead of dashes\n",
    "    start_clean = start_date.replace(\"-\", \"_\")\n",
    "    end_clean = end_date.replace(\"-\", \"_\")\n",
    "\n",
    "    # üîπ Build directory name with dates + interval\n",
    "    date_dir = f\"{start_clean}_to_{end_clean}_{interval}\"\n",
    "    data_dir = os.path.join(BASE_DATA_DIR, date_dir)\n",
    "    if save_to_file:\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, f\"{symbol}.csv\")  # üîπ File name without date\n",
    "\n",
    "    # Load from cache if available\n",
    "    if save_to_file and os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "        print(f\"üìÑ Loaded {symbol} data from file ({len(df)} rows)\")\n",
    "        return df\n",
    "\n",
    "    # Fetch live data\n",
    "    print(f\"Calling live API for {symbol}\")\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    df = ticker.history(start=start_date, end=end_date_exclusive, interval=interval)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è No data found for {symbol}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df.index = df.index.tz_localize(None)  # remove timezone if present\n",
    "    df = df.rename(columns={\n",
    "        \"Open\": \"open\",\n",
    "        \"High\": \"high\",\n",
    "        \"Low\": \"low\",\n",
    "        \"Close\": \"close\",\n",
    "        \"Volume\": \"volume\"\n",
    "    })\n",
    "\n",
    "    df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "    # Save to cache if enabled\n",
    "    if save_to_file:\n",
    "        df.to_csv(file_path)\n",
    "        print(f\"‚úÖ Saved {symbol} data to {file_path} ({len(df)} rows)\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35161e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Index return using Yahoo Finance (SPY as proxy for S&P 500)\n",
    "end_date_dt = datetime.strptime(END_DATE, TIME_FORMAT) if END_DATE else datetime.now()\n",
    "# start_date = (end_date_dt - timedelta(days=TIME_PERIOD_DAYS)).strftime(TIME_FORMAT)\n",
    "\n",
    "# Subtract exactly 1 year (calendar-aware, handles leap years)\n",
    "start_date_dt = (end_date_dt - relativedelta(years=1)) + timedelta(days=1)\n",
    "\n",
    "start_date = start_date_dt.strftime(TIME_FORMAT)\n",
    "end_date = end_date_dt.strftime(TIME_FORMAT)\n",
    "\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "index_df = get_yfinance_data(INDEX_SYMBOL, start_date=start_date, end_date=end_date)\n",
    "\n",
    "# Ensure 'close' is float\n",
    "index_df[\"close\"] = index_df[\"close\"].astype(float)\n",
    "\n",
    "# Compute daily percent change\n",
    "index_df[\"Percent Change\"] = index_df[\"close\"].pct_change()\n",
    "\n",
    "# Compute cumulative return\n",
    "index_return = (index_df[\"Percent Change\"] + 1).cumprod().iloc[-1]\n",
    "\n",
    "print(f\"S&P 500 proxy return (SPY): {index_return:.2f}x\")\n",
    "print(index_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yahoo_data(tickers):\n",
    "    \"\"\"\n",
    "    Fetch data once and store in yahoo_data dictionary.\n",
    "    Writes passed and failed tickers to text files.\n",
    "    \"\"\"\n",
    "    yahoo_data = {}\n",
    "    successful_tickers = []\n",
    "    failed_tickers = []\n",
    "\n",
    "    for symbol in tickers:\n",
    "        df = get_yfinance_data(symbol, start_date=start_date, end_date=end_date)\n",
    "\n",
    "        if df.empty or \"close\" not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Skipping {symbol}: No data or bad format\")\n",
    "            failed_tickers.append(symbol)\n",
    "            continue\n",
    "\n",
    "        yahoo_data[symbol] = df\n",
    "        successful_tickers.append(symbol)\n",
    "        print(f\"‚úÖ Loaded {symbol} successfully\")\n",
    "\n",
    "    # Write failed tickers to file\n",
    "    with open(\"failed_tickers.txt\", \"w\") as f:\n",
    "        for ticker in failed_tickers:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "\n",
    "    # Write successful tickers to file (optional)\n",
    "    with open(\"successful_tickers.txt\", \"w\") as f:\n",
    "        for ticker in successful_tickers:\n",
    "            f.write(f\"{ticker}\\n\")\n",
    "\n",
    "    print(f\"\\nüìÑ {len(successful_tickers)} tickers loaded successfully and saved to successful_tickers.txt\")\n",
    "    print(f\"üìÑ {len(failed_tickers)} tickers failed and saved to failed_tickers.txt\")\n",
    "    \n",
    "    return yahoo_data, successful_tickers, failed_tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rs(index_return, start_date, end_date, index_symbol=\"SPY\"):\n",
    "    \"\"\"\n",
    "    Calculate RS scores using preloaded yahoo_data and precomputed index_return.\n",
    "    Only processes successful tickers from load_yahoo_data().\n",
    "    \"\"\"\n",
    "    global yahoo_data, successful_tickers\n",
    "\n",
    "    returns_multiples = []\n",
    "\n",
    "    for symbol in successful_tickers:\n",
    "        if symbol == INDEX_SYMBOL:\n",
    "            continue  # skip the index itself\n",
    "\n",
    "        df_subset = yahoo_data[symbol].loc[start_date:end_date].copy()\n",
    "        df_subset[\"close\"] = df_subset[\"close\"].astype(float)\n",
    "        df_subset[\"Percent Change\"] = df_subset[\"close\"].pct_change()\n",
    "        stock_return = (df_subset[\"Percent Change\"] + 1).cumprod().iloc[-1]\n",
    "        rs_score = stock_return / index_return\n",
    "        returns_multiples.append(rs_score)\n",
    "        print(f\"‚úÖ {symbol} processed ‚Äì RS Score: {rs_score:.2f}\")\n",
    "\n",
    "    return returns_multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce51030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_rs_top_df(successful_tickers, returns_multiples, top_quantile=0.7):\n",
    "    \"\"\"\n",
    "    Create RS Rating DataFrame and filter top stocks by percentile.\n",
    "    \n",
    "    Args:\n",
    "        successful_tickers (list): List of tickers successfully loaded.\n",
    "        returns_multiples (list): Corresponding return multiples for tickers.\n",
    "        top_quantile (float): Quantile threshold for top RS stocks (default 0.7).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Top RS stocks with columns ['Ticker', 'Returns_Multiple', 'RS_Rating'].\n",
    "    \"\"\"\n",
    "    rs_df = pd.DataFrame({\n",
    "        'Ticker': successful_tickers,\n",
    "        'Returns_Multiple': returns_multiples\n",
    "    })\n",
    "    rs_df['RS_Rating'] = rs_df['Returns_Multiple'].rank(pct=True) * 100\n",
    "\n",
    "    # Filter top stocks by RS_Rating quantile\n",
    "    threshold = rs_df['RS_Rating'].quantile(top_quantile)\n",
    "    rs_top_df = rs_df[rs_df['RS_Rating'] >= threshold].reset_index(drop=True)\n",
    "    return rs_top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_minervini_report(yahoo_data, date_str, rs_top_df, export_prefix=\"Minervini\"):\n",
    "    \"\"\"\n",
    "    Generate Minervini report for top RS stocks without FutureWarning.\n",
    "    \"\"\"\n",
    "    rows = []  # Collect row dicts here instead of using concat in the loop\n",
    "\n",
    "    for stock in rs_top_df['Ticker']:\n",
    "        try:\n",
    "            df = yahoo_data.get(stock)\n",
    "            if df is None or df.empty:\n",
    "                print(f\"‚ö†Ô∏è No data for {stock}, skipping\")\n",
    "                continue\n",
    "\n",
    "            df.columns = [col.split(\". \")[-1] for col in df.columns]\n",
    "            df[['close', 'high', 'low']] = df[['close', 'high', 'low']].astype(float)\n",
    "\n",
    "            df['SMA_50'] = df['close'].rolling(50).mean()\n",
    "            df['SMA_150'] = df['close'].rolling(150).mean()\n",
    "            df['SMA_200'] = df['close'].rolling(200).mean()\n",
    "\n",
    "            currentClose = round(df['close'].iloc[-1], 2)\n",
    "            SMA_50 = round(df['SMA_50'].iloc[-1], 2)\n",
    "            SMA_150 = round(df['SMA_150'].iloc[-1], 2)\n",
    "            SMA_200 = round(df['SMA_200'].iloc[-1], 2)\n",
    "            low_52week = round(df['low'].iloc[-260:].min(), 2)\n",
    "            high_52week = round(df['high'].iloc[-260:].max(), 2)\n",
    "\n",
    "            Returns_Multiple = round(rs_top_df.loc[rs_top_df['Ticker'] == stock, 'Returns_Multiple'].iloc[0], 2)\n",
    "            RS_Rating = round(rs_top_df.loc[rs_top_df['Ticker'] == stock, 'RS_Rating'].iloc[0])\n",
    "\n",
    "            if pd.isnull([SMA_50, SMA_150, SMA_200]).any():\n",
    "                status_msg = f\"‚ùå Skipped {stock}: Not enough data for SMAs ({len(df)} rows)\"\n",
    "            else:\n",
    "                SMA_200_20 = df['SMA_200'].iloc[-20] if len(df) >= 220 else 0\n",
    "                conditions = [\n",
    "                    currentClose > SMA_150 > SMA_200,\n",
    "                    SMA_150 > SMA_200,\n",
    "                    SMA_200 > SMA_200_20,\n",
    "                    SMA_50 > SMA_150 > SMA_200,\n",
    "                    currentClose > SMA_50,\n",
    "                    currentClose >= 1.3 * low_52week,\n",
    "                    currentClose >= 0.75 * high_52week\n",
    "                ]\n",
    "                if all(conditions):\n",
    "                    status_msg = \"‚úÖ Passed Minervini\"\n",
    "                else:\n",
    "                    failed = [str(i+1) for i, c in enumerate(conditions) if not c]\n",
    "                    status_msg = f\"‚ùå Failed Minervini conditions: {', '.join(failed)}\"\n",
    "\n",
    "            # Add row dict to list\n",
    "            rows.append({\n",
    "                'Stock': stock,\n",
    "                'Price': currentClose,\n",
    "                '50 Day MA': SMA_50,\n",
    "                '150 Day MA': SMA_150,\n",
    "                '200 Day MA': SMA_200,\n",
    "                '52 Week Low': low_52week,\n",
    "                '52 Week High': high_52week,\n",
    "                'Returns_Multiple': Returns_Multiple,\n",
    "                'RS_Rating': RS_Rating,\n",
    "                'Status': status_msg\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not process {stock}: {e}\")\n",
    "\n",
    "    # Create DataFrame once at the end\n",
    "    detailedExportList = pd.DataFrame(rows)\n",
    "\n",
    "    # Split and sort\n",
    "    passed_df_sorted = detailedExportList[detailedExportList['Status'] == \"‚úÖ Passed Minervini\"] \\\n",
    "        .sort_values(by=['Returns_Multiple', 'Stock'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    failed_df_sorted = detailedExportList[detailedExportList['Status'] != \"‚úÖ Passed Minervini\"] \\\n",
    "        .sort_values(by=['Returns_Multiple', 'Stock'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # Export\n",
    "    date_file_str = get_file_date_str(date_str)\n",
    "    passed_file = f\"{export_prefix}_Passed_{date_file_str}.csv\"\n",
    "    failed_file = f\"{export_prefix}_Failed_{date_file_str}.csv\"\n",
    "\n",
    "    passed_df_sorted.to_csv(passed_file, index=False)\n",
    "    failed_df_sorted.to_csv(failed_file, index=False)\n",
    "\n",
    "    print(f\"\\nüìÑ {len(passed_df_sorted)} stocks passed Minervini, saved to {passed_file}\")\n",
    "    print(f\"üìÑ {len(failed_df_sorted)} stocks failed Minervini, saved to {failed_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f59927",
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_data, successful_tickers, failed_tickers = load_yahoo_data(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_multiples = calculate_rs(index_return, start_date, end_date)\n",
    "print(f\"\\nCalculated RS scores for {len(successful_tickers)} tickers.\")\n",
    "\n",
    "rs_top_df = create_rs_top_df(successful_tickers, returns_multiples)\n",
    "generate_minervini_report(yahoo_data, END_DATE, rs_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_valid_price(ticker: str, date_str: str) -> float | None:\n",
    "    \"\"\"\n",
    "    Fetch the closing price for a ticker on a given date.\n",
    "    Returns None if data is missing or invalid.\n",
    "    \"\"\"\n",
    "    df = get_yfinance_data(ticker, date_str, date_str, save_to_file=False)\n",
    "    if df.empty or \"close\" not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è No data for {ticker} on {date_str}\")\n",
    "        return None\n",
    "\n",
    "    price = df[\"close\"].iloc[0]\n",
    "    if pd.isna(price) or price <= 0:\n",
    "        print(f\"‚ö†Ô∏è Invalid price for {ticker} on {date_str}: {price}\")\n",
    "        return None\n",
    "\n",
    "    return price\n",
    "\n",
    "def backtest_minervini_from_file(passed_file, invest_date_str, exit_date_str, investment_per_stock=1000, interval=\"1d\"):\n",
    "    \"\"\"\n",
    "    Backtest Minervini strategy using previously saved passed tickers CSV.\n",
    "    Invest `investment_per_stock` in each VALID ticker at END_DATE price and\n",
    "    calculate returns 1 year later using get_yfinance_data().\n",
    "\n",
    "    Only tickers with valid invest and exit prices are counted in total_invested.\n",
    "    \"\"\"\n",
    "    passed_df = pd.read_csv(passed_file)\n",
    "    if passed_df.empty:\n",
    "        print(\"‚ö†Ô∏è No tickers in passed file.\")\n",
    "        return None\n",
    "\n",
    "    total_portfolio_value = 0.0\n",
    "    invested_count = 0\n",
    "    invested_tickers = []\n",
    "\n",
    "    for ticker in passed_df[\"Stock\"]:\n",
    "        try:\n",
    "            # --- Get invest price ---\n",
    "            invest_price = get_valid_price(ticker, invest_date_str)\n",
    "            if invest_price is None:\n",
    "                continue\n",
    "\n",
    "            # --- Get exit price ---\n",
    "            exit_price = get_valid_price(ticker, exit_date_str)\n",
    "            if exit_price is None:\n",
    "                continue\n",
    "\n",
    "            # --- Compute final value and count the investment ---\n",
    "            shares = investment_per_stock / invest_price\n",
    "            final_value = shares * exit_price\n",
    "            total_portfolio_value += final_value\n",
    "            invested_count += 1\n",
    "            invested_tickers.append(ticker)\n",
    "\n",
    "            print(f\"‚úÖ {ticker}: Bought at {invest_price:.2f}, Sold at {exit_price:.2f}, Value: {final_value:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {ticker}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Only count invested tickers\n",
    "    total_invested = invested_count * investment_per_stock\n",
    "    total_return_pct = ((total_portfolio_value / total_invested - 1) * 100) if total_invested > 0 else 0.0\n",
    "\n",
    "    print(\"\\nüìä Backtest Summary\")\n",
    "    print(f\"Tickers in passed file: {len(passed_df)}\")\n",
    "    print(f\"Tickers actually invested in: {invested_count}\")\n",
    "    print(f\"Total invested: ${float(total_invested):,.2f}\")\n",
    "    print(f\"Portfolio value after 1 year: ${float(total_portfolio_value):,.2f}\")\n",
    "    print(f\"Total return: {float(total_return_pct):.2f}%\")\n",
    "\n",
    "    return {\n",
    "        \"invested_count\": invested_count,\n",
    "        \"invested_tickers\": invested_tickers,\n",
    "        \"total_invested\": int(total_invested),  # safe since it's always multiple of $1,000\n",
    "        \"portfolio_value\": round(float(total_portfolio_value), 2),\n",
    "        \"total_return_pct\": round(float(total_return_pct), 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_minervini_from_file(\"Minervini_Passed_2025_09_18.csv\", start_date, end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
